{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iEIzeqXOH6Nv"
      },
      "source": [
        "# Start"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 754
        },
        "id": "bJvctrNTMPzx",
        "outputId": "7306ecc8-f704-4e32-fd3d-96cdceb1df99"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting gym==0.19.0\n",
            "  Downloading gym-0.19.0.tar.gz (1.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 7.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.7/dist-packages (from gym==0.19.0) (1.21.6)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym==0.19.0) (1.5.0)\n",
            "Building wheels for collected packages: gym\n",
            "  Building wheel for gym (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gym: filename=gym-0.19.0-py3-none-any.whl size=1663116 sha256=c8e8ed81ce74fa9c54437a63b6f65d3311d05eb1e5fb914ea81f03204c67080c\n",
            "  Stored in directory: /root/.cache/pip/wheels/ef/9d/70/8bea53f7edec2fdb4f98d9d64ac9f11aea95dfcb98099d7712\n",
            "Successfully built gym\n",
            "Installing collected packages: gym\n",
            "  Attempting uninstall: gym\n",
            "    Found existing installation: gym 0.25.2\n",
            "    Uninstalling gym-0.25.2:\n",
            "      Successfully uninstalled gym-0.25.2\n",
            "Successfully installed gym-0.19.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "gym"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gym[atari] in /usr/local/lib/python3.7/dist-packages (0.19.0)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym[atari]) (1.5.0)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.7/dist-packages (from gym[atari]) (1.21.6)\n",
            "Requirement already satisfied: opencv-python>=3. in /usr/local/lib/python3.7/dist-packages (from gym[atari]) (4.6.0.66)\n",
            "Collecting atari-py==0.2.6\n",
            "  Downloading atari_py-0.2.6-cp37-cp37m-manylinux1_x86_64.whl (2.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.8 MB 7.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from atari-py==0.2.6->gym[atari]) (1.15.0)\n",
            "\u001b[33mWARNING: The candidate selected for download or install is a yanked version: 'atari-py' candidate (version 0.2.6 at https://files.pythonhosted.org/packages/8f/ba/1d22e9d2f332f07aaa57041f5dd569c2cb40a92bd6374a0b743ec3dfae97/atari_py-0.2.6-cp37-cp37m-manylinux1_x86_64.whl#sha256=d9e2c25d39783867c2f29d1dd9d3a659fc56036456d07dc9efe8bd7bb31a07d7 (from https://pypi.org/simple/atari-py/))\n",
            "Reason for being yanked: re-release with new wheels\u001b[0m\n",
            "Installing collected packages: atari-py\n",
            "  Attempting uninstall: atari-py\n",
            "    Found existing installation: atari-py 0.2.9\n",
            "    Uninstalling atari-py-0.2.9:\n",
            "      Successfully uninstalled atari-py-0.2.9\n",
            "Successfully installed atari-py-0.2.6\n"
          ]
        }
      ],
      "source": [
        "%pip install --upgrade gym==0.19.0\n",
        "%pip install --upgrade gym[atari]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4WPicL0CCqev"
      },
      "source": [
        "# Wrappers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yf7Mnfr-Bz-G"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Useful wrappers taken from OpenAI (https://github.com/openai/baselines)\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "import gym\n",
        "from gym import spaces\n",
        "import cv2\n",
        "\n",
        "import random\n",
        "\n",
        "cv2.ocl.setUseOpenCL(False)\n",
        "\n",
        "\n",
        "class NoopResetEnv(gym.Wrapper):\n",
        "    def __init__(self, env, noop_max=30):\n",
        "        \"\"\"Sample initial states by taking random number of no-ops on reset.\n",
        "        No-op is assumed to be action 0.\n",
        "        \"\"\"\n",
        "        gym.Wrapper.__init__(self, env)\n",
        "        self.noop_max = noop_max\n",
        "        self.override_num_noops = None\n",
        "        self.noop_action = 0\n",
        "        assert env.unwrapped.get_action_meanings()[0] == \"NOOP\"\n",
        "\n",
        "    def reset(self, **kwargs):\n",
        "        \"\"\" Do no-op action for a number of steps in [1, noop_max].\"\"\"\n",
        "        self.env.reset(**kwargs)\n",
        "        if self.override_num_noops is not None:\n",
        "            noops = self.override_num_noops\n",
        "        else:\n",
        "            noops = random.randint(\n",
        "                1, self.noop_max + 1\n",
        "            )  # pylint: disable=E1101\n",
        "        assert noops > 0\n",
        "        obs = None\n",
        "        for _ in range(noops):\n",
        "            obs, _, done, _ = self.env.step(self.noop_action)\n",
        "            if done:\n",
        "                obs = self.env.reset(**kwargs)\n",
        "        return obs\n",
        "\n",
        "    def step(self, ac):\n",
        "        return self.env.step(ac)\n",
        "\n",
        "\n",
        "class FireResetEnv(gym.Wrapper):\n",
        "    def __init__(self, env):\n",
        "        \"\"\"Take action on reset for environments that are fixed until firing.\"\"\"\n",
        "        gym.Wrapper.__init__(self, env)\n",
        "        assert env.unwrapped.get_action_meanings()[1] == \"FIRE\"\n",
        "        assert len(env.unwrapped.get_action_meanings()) >= 3\n",
        "\n",
        "    def reset(self, **kwargs):\n",
        "        self.env.reset(**kwargs)\n",
        "        obs, _, done, _ = self.env.step(1)\n",
        "        if done:\n",
        "            self.env.reset(**kwargs)\n",
        "        obs, _, done, _ = self.env.step(2)\n",
        "        if done:\n",
        "            self.env.reset(**kwargs)\n",
        "        return obs\n",
        "\n",
        "    def step(self, ac):\n",
        "        return self.env.step(ac)\n",
        "\n",
        "\n",
        "class EpisodicLifeEnv(gym.Wrapper):\n",
        "    def __init__(self, env):\n",
        "        \"\"\"Make end-of-life == end-of-episode, but only reset on true game over.\n",
        "        Done by DeepMind for the DQN and co. since it helps value estimation.\n",
        "        \"\"\"\n",
        "        gym.Wrapper.__init__(self, env)\n",
        "        self.lives = 0\n",
        "        self.was_real_done = True\n",
        "\n",
        "    def step(self, action):\n",
        "        obs, reward, done, info = self.env.step(action)\n",
        "        self.was_real_done = done\n",
        "        # check current lives, make loss of life terminal,\n",
        "        # then update lives to handle bonus lives\n",
        "        lives = self.env.unwrapped.ale.lives()\n",
        "        if lives < self.lives and lives > 0:\n",
        "            # for Qbert sometimes we stay in lives == 0 condtion for a few frames\n",
        "            # so its important to keep lives > 0, so that we only reset once\n",
        "            # the environment advertises done.\n",
        "            done = True\n",
        "        self.lives = lives\n",
        "        return obs, reward, done, info\n",
        "\n",
        "    def reset(self, **kwargs):\n",
        "        \"\"\"Reset only when lives are exhausted.\n",
        "        This way all states are still reachable even though lives are episodic,\n",
        "        and the learner need not know about any of this behind-the-scenes.\n",
        "        \"\"\"\n",
        "        if self.was_real_done:\n",
        "            obs = self.env.reset(**kwargs)\n",
        "        else:\n",
        "            # no-op step to advance from terminal/lost life state\n",
        "            obs, _, _, _ = self.env.step(0)\n",
        "        self.lives = self.env.unwrapped.ale.lives()\n",
        "        return obs\n",
        "\n",
        "\n",
        "class MaxAndSkipEnv(gym.Wrapper):\n",
        "    def __init__(self, env, skip=4):\n",
        "        \"\"\"Return only every `skip`-th frame\"\"\"\n",
        "        gym.Wrapper.__init__(self, env)\n",
        "        # most recent raw observations (for max pooling across time steps)\n",
        "        self._obs_buffer = np.zeros((2,) + env.observation_space.shape, dtype=np.uint8)\n",
        "        self._skip = skip\n",
        "\n",
        "    def reset(self):\n",
        "        return self.env.reset()\n",
        "\n",
        "    def step(self, action):\n",
        "        \"\"\"Repeat action, sum reward, and max over last observations.\"\"\"\n",
        "        total_reward = 0.0\n",
        "        done = None\n",
        "        for i in range(self._skip):\n",
        "            obs, reward, done, info = self.env.step(action)\n",
        "            if i == self._skip - 2:\n",
        "                self._obs_buffer[0] = obs\n",
        "            if i == self._skip - 1:\n",
        "                self._obs_buffer[1] = obs\n",
        "            total_reward += reward\n",
        "            if done:\n",
        "                break\n",
        "        # Note that the observation on the done=True frame\n",
        "        # doesn't matter\n",
        "        max_frame = self._obs_buffer.max(axis=0)\n",
        "\n",
        "        return max_frame, total_reward, done, info\n",
        "\n",
        "    def reset(self, **kwargs):\n",
        "        return self.env.reset(**kwargs)\n",
        "\n",
        "\n",
        "class ClipRewardEnv(gym.RewardWrapper):\n",
        "    def __init__(self, env):\n",
        "        gym.RewardWrapper.__init__(self, env)\n",
        "\n",
        "    def reward(self, reward):\n",
        "        \"\"\"Bin reward to {+1, 0, -1} by its sign.\"\"\"\n",
        "        return np.sign(reward)\n",
        "\n",
        "\n",
        "class WarpFrame(gym.ObservationWrapper):\n",
        "    def __init__(self, env):\n",
        "        \"\"\"Warp frames to 84x84 as done in the Nature paper and later work.\n",
        "        Expects inputs to be of shape height x width x num_channels\n",
        "        \"\"\"\n",
        "        gym.ObservationWrapper.__init__(self, env)\n",
        "        self.width = 84\n",
        "        self.height = 84\n",
        "        self.observation_space = spaces.Box(\n",
        "            low=0, high=255, shape=(self.height, self.width, 1), dtype=np.uint8\n",
        "        )\n",
        "\n",
        "    def observation(self, frame):\n",
        "        frame = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n",
        "        frame = cv2.resize(\n",
        "            frame, (self.width, self.height), interpolation=cv2.INTER_AREA\n",
        "        )\n",
        "        return frame[:, :, None]\n",
        "\n",
        "\n",
        "class FrameStack(gym.Wrapper):\n",
        "    def __init__(self, env, k):\n",
        "        \"\"\"Stack k last frames.\n",
        "        Returns lazy array, which is much more memory efficient.\n",
        "        Expects inputs to be of shape num_channels x height x width.\n",
        "        \"\"\"\n",
        "        gym.Wrapper.__init__(self, env)\n",
        "        self.k = k\n",
        "        self.frames = deque([], maxlen=k)\n",
        "        shp = env.observation_space.shape\n",
        "        self.observation_space = spaces.Box(\n",
        "            low=0, high=255, shape=(shp[0] * k, shp[1], shp[2]), dtype=np.uint8\n",
        "        )\n",
        "\n",
        "    def reset(self):\n",
        "        ob = self.env.reset()\n",
        "        for _ in range(self.k):\n",
        "            self.frames.append(ob)\n",
        "        return self._get_ob()\n",
        "\n",
        "    def step(self, action):\n",
        "        ob, reward, done, info = self.env.step(action)\n",
        "        self.frames.append(ob)\n",
        "        return self._get_ob(), reward, done, info\n",
        "\n",
        "    def _get_ob(self):\n",
        "        assert len(self.frames) == self.k\n",
        "        return LazyFrames(list(self.frames))\n",
        "\n",
        "\n",
        "class ScaledFloatFrame(gym.ObservationWrapper):\n",
        "    def __init__(self, env):\n",
        "        gym.ObservationWrapper.__init__(self, env)\n",
        "\n",
        "    def observation(self, observation):\n",
        "        # careful! This undoes the memory optimization, use\n",
        "        # with smaller replay buffers only.\n",
        "        return np.array(observation).astype(np.float32) / 255.0\n",
        "\n",
        "\n",
        "class LazyFrames(object):\n",
        "    def __init__(self, frames):\n",
        "        \"\"\"This object ensures that common frames between the observations are only stored once.\n",
        "        It exists purely to optimize memory usage which can be huge for DQN's 1M frames replay\n",
        "        buffers.\"\"\"\n",
        "        self._frames = frames\n",
        "\n",
        "    def __array__(self, dtype=None):\n",
        "        out = np.concatenate(self._frames, axis=0)\n",
        "        if dtype is not None:\n",
        "            out = out.astype(dtype)\n",
        "        return out\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self._frames)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        return self._frames[i]\n",
        "\n",
        "\n",
        "class PyTorchFrame(gym.ObservationWrapper):\n",
        "    \"\"\"Image shape to num_channels x height x width\"\"\"\n",
        "\n",
        "    def __init__(self, env):\n",
        "        super(PyTorchFrame, self).__init__(env)\n",
        "        shape = self.observation_space.shape\n",
        "        self.observation_space = gym.spaces.Box(\n",
        "            low=0.0, high=1.0, shape=(shape[-1], shape[0], shape[1]), dtype=np.uint8\n",
        "        )\n",
        "\n",
        "    def observation(self, observation):\n",
        "        return np.rollaxis(observation, 2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "54DTXdQdDF_4"
      },
      "source": [
        "# Reply Buffer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "il5nuo0lC6T2"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "class ReplayBuffer:\n",
        "    \"\"\"\n",
        "    Simple storage for transitions from an environment.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, size):\n",
        "        \"\"\"\n",
        "        Initialise a buffer of a given size for storing transitions\n",
        "        :param size: the maximum number of transitions that can be stored\n",
        "        \"\"\"\n",
        "        self._storage = []\n",
        "        self._maxsize = size\n",
        "        self._next_idx = 0\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self._storage)\n",
        "\n",
        "    def add(self, state, action, reward, next_state, done):\n",
        "        \"\"\"\n",
        "        Add a transition to the buffer. Old transitions will be overwritten if the buffer is full.\n",
        "        :param state: the agent's initial state\n",
        "        :param action: the action taken by the agent\n",
        "        :param reward: the reward the agent received\n",
        "        :param next_state: the subsequent state\n",
        "        :param done: whether the episode terminated\n",
        "        \"\"\"\n",
        "        data = (state, action, reward, next_state, done)\n",
        "\n",
        "        if self._next_idx >= len(self._storage):\n",
        "            self._storage.append(data)\n",
        "        else:\n",
        "            self._storage[self._next_idx] = data\n",
        "        self._next_idx = (self._next_idx + 1) % self._maxsize\n",
        "\n",
        "    def _encode_sample(self, indices):\n",
        "        states, actions, rewards, next_states, dones = [], [], [], [], []\n",
        "        for i in indices:\n",
        "            data = self._storage[i]\n",
        "            state, action, reward, next_state, done = data\n",
        "            states.append(np.array(state, copy=False))\n",
        "            actions.append(action)\n",
        "            rewards.append(reward)\n",
        "            next_states.append(np.array(next_state, copy=False))\n",
        "            dones.append(done)\n",
        "        return (\n",
        "            np.array(states),\n",
        "            np.array(actions),\n",
        "            np.array(rewards),\n",
        "            np.array(next_states),\n",
        "            np.array(dones),\n",
        "        )\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        \"\"\"\n",
        "        Randomly sample a batch of transitions from the buffer.\n",
        "        :param batch_size: the number of transitions to sample\n",
        "        :return: a mini-batch of sampled transitions\n",
        "        \"\"\"\n",
        "        indices = np.random.randint(0, len(self._storage) - 1, size=batch_size)\n",
        "        return self._encode_sample(indices)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RzO_qNfHDNur"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uX__2G3dDJsv"
      },
      "outputs": [],
      "source": [
        "from gym import spaces\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class DQN(nn.Module):\n",
        "    \"\"\"\n",
        "    A basic implementation of a Deep Q-Network. The architecture is the same as that described in the\n",
        "    Nature DQN paper.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, observation_space: spaces.Box, action_space: spaces.Discrete):\n",
        "        \"\"\"\n",
        "        Initialise the DQN\n",
        "        :param observation_space: the state space of the environment\n",
        "        :param action_space: the action space of the environment\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        assert (\n",
        "            type(observation_space) == spaces.Box\n",
        "        ), \"observation_space must be of type Box\"\n",
        "        assert (\n",
        "            len(observation_space.shape) == 3\n",
        "        ), \"observation space must have the form channels x width x height\"\n",
        "        assert (\n",
        "            type(action_space) == spaces.Discrete\n",
        "        ), \"action_space must be of type Discrete\"\n",
        "        \n",
        "        \n",
        "        #  input image is 84 x 84 X 4 \n",
        "        #  first hidden layer convolves 16 8 x 8 filters with stride 4\n",
        "        #  it means we do a 8 x 8 convolution without padding and stride=4\n",
        "        #  stride shrinks the size to ceil((n+f-1)/s) where ’n’ is input dimensions ‘f’ is filter size and ‘s’ is stride length\n",
        "        #  the input image will be strinked to ((84+8-1)/4)\n",
        "        #  output layer is fully connected with single output for each valid action\n",
        "        \n",
        "        # num out features in current layer is equal to num in features in lext layer\n",
        "  \n",
        "        # self.convolutions = nn.Sequential(\n",
        "            \n",
        "        #     #  first hidden layer convolves 16 8 x 8 filters with stride 4\n",
        "        #     nn.Conv2d(observation_space.shape[0], 32, 8, stride=4),\n",
        "        #     nn.ReLU(),\n",
        "        #     #  second layer convolves 32 4x4 filters with stride 2 -> num in_features is 32\n",
        "        #     #  because num out_features in current layer has to be equal to num in_features in lext layer\n",
        "        #     #  i then said num out_fearures for the first layer is 32, idk\n",
        "        #     #  So for the num out_features i said because Deep Q-Learning uses 2 neural networks \n",
        "        #     #  so at every step weights from the main network are copied to the target network\n",
        "        #     #  so at each step 2 copies of the input are produced \n",
        "        #     #  so the output will be twice the input\n",
        "        #     #  idk if this understanding is correct\n",
        "        #     nn.Conv2d(32, 64, 4, stride=2),\n",
        "        #     nn.ReLU(),\n",
        "        #     #  final hidden layer is fully connected with 256 rectifier units -> mnih paper has 512 \n",
        "        #     nn.Conv2d(64, 64, 3, stride=1),\n",
        "        #     nn.ReLU(),\n",
        "        # )\n",
        "\n",
        "        # self.fully_connected_layer = nn.Sequential(\n",
        "        #     nn.Linear(in_features=64*7*7 , out_features=512), #512 not 256 \n",
        "        #     nn.ReLU(),\n",
        "        #     nn.Linear(in_features=512, out_features=action_space.n)\n",
        "        # )\n",
        "        self.conv1 = nn.Conv2d(observation_space.shape[0],32, 8,stride=4)\n",
        "        #self.maxpool1 = MaxPool2d(kernel_size=(1, 1), stride=(2, 2))\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4,stride=2)\n",
        "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3,stride=1)\n",
        "        #self.maxpool2 = MaxPool2d(kernel_size=1, stride=(2, 2))\n",
        "        self.fc1 = nn.Linear(in_features=64*7*7, out_features=512)\n",
        "        self.fc2 = nn.Linear(in_features=512, out_features=action_space.n)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # output = self.convolutions(x)\n",
        "        # output = self.fully_connected_layer(output.view(x.size()[0],-1))\n",
        "        # return output\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = F.relu(self.conv3(x))\n",
        "        x = x.view(-1, 64 * 7 * 7)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dXFhF9-UDTJI"
      },
      "source": [
        "# Agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PCOwDF_1DP2S"
      },
      "outputs": [],
      "source": [
        "from torch.optim import Adam\n",
        "from torch import nn\n",
        "import torch\n",
        "\n",
        "import torch.nn.functional as F\n",
        "\n",
        "device = \"cuda\"\n",
        "\n",
        "\n",
        "class DQNAgent:\n",
        "    def __init__(\n",
        "        self,\n",
        "        observation_space: spaces.Box,\n",
        "        action_space: spaces.Discrete,\n",
        "        replay_buffer: ReplayBuffer,\n",
        "        use_double_dqn,\n",
        "        lr,\n",
        "        batch_size,\n",
        "        gamma,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Initialise the DQN algorithm using the Adam optimiser\n",
        "        :param action_space: the action space of the environment\n",
        "        :param observation_space: the state space of the environment\n",
        "        :param replay_buffer: storage for experience replay\n",
        "        :param lr: the learning rate for Adam\n",
        "        :param batch_size: the batch size\n",
        "        :param gamma: the discount factor\n",
        "        \"\"\"\n",
        "\n",
        "        # TODO: Initialise agent's networks, optimiser and replay buffer -> Done \n",
        "        self.lr = lr\n",
        "        self.replay_buffer = replay_buffer\n",
        "        self.gamma = gamma\n",
        "        self.batch_size = batch_size\n",
        "        self.use_double_dqn = use_double_dqn\n",
        "        #agents networks\n",
        "        self.target_network = DQN(observation_space,action_space).to(device)\n",
        "        self.policy_network = DQN(observation_space,action_space).to(device)\n",
        "        #agents optimiser \n",
        "        self.optimiser = Adam(self.policy_network.parameters(),lr=self.lr)\n",
        "        \n",
        "        # raise NotImplementedError\n",
        "\n",
        "    def optimise_td_loss(self):\n",
        "        \"\"\"\n",
        "        Optimise the TD-error over a single minibatch of transitions\n",
        "        :return: the loss\n",
        "        \"\"\"\n",
        "        # TODO -> done\n",
        "        #   Optimise the TD-error over a single minibatch of transitions\n",
        "        #   Sample the minibatch from the replay-memory\n",
        "        #   using done (as a float) instead of if statement\n",
        "        #   return loss\n",
        "\n",
        "        # get batch from replay buffer and convert info into tensors\n",
        "        states, actions, rewards, next_states, dones = self.replay_buffer.sample(self.batch_size)\n",
        "        tensor_states = torch.from_numpy(states/255.0).float().to(device)\n",
        "        tensor_next_states = torch.from_numpy(next_states/255.0).float().to(device)\n",
        "        tensor_actions = torch.from_numpy(actions).long().to(device)\n",
        "        tensor_rewards = torch.from_numpy(rewards).float().to(device)\n",
        "        tensor_dones = torch.from_numpy(dones).float().to(device)\n",
        "\n",
        "        # don't track gradients\n",
        "        with torch.no_grad():\n",
        "            # calculate target\n",
        "            estimate = None\n",
        "            if self.use_double_dqn:\n",
        "                # get next action from the other network\n",
        "                _, next_action = self.policy_network(tensor_states).max(1)\n",
        "                # get next q from target network\n",
        "                estimate = self.target_network(tensor_next_states).gather(1, next_action.unsqueeze(1)).squeeze()\n",
        "            else:\n",
        "                estimate = None\n",
        "              \n",
        "                # get next q from target network\n",
        "                \n",
        "\n",
        "        estimate = self.target_network(tensor_next_states).max(1)\n",
        "\n",
        "        c = self.gamma * estimate[0]\n",
        "        target = tensor_rewards + (1 - tensor_dones) * c\n",
        "\n",
        "        # backpropagation\n",
        "        new_estimate = self.policy_network(tensor_states).gather(1, tensor_actions.unsqueeze(1)).squeeze()\n",
        "        loss = nn.functional.l1_loss(new_estimate, target)\n",
        "        self.optimiser.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimiser.step()\n",
        "\n",
        "        # # remove from GPU\n",
        "        # del tensor_states\n",
        "        # del tensor_next_states\n",
        "\n",
        "        return loss.item()\n",
        "\n",
        "    def update_target_network(self):\n",
        "        \"\"\"\n",
        "        Update the target Q-network by copying the weights from the current Q-network\n",
        "        \"\"\"\n",
        "        # TODO update target_network parameters with policy_network parameters -> Done \n",
        "        self.target_network.load_state_dict(self.policy_network.state_dict())\n",
        "\n",
        "    def act(self, state: np.ndarray):\n",
        "        \"\"\"\n",
        "        Select an action greedily from the Q-network given the state\n",
        "        :param state: the current state\n",
        "        :return: the action to take\n",
        "        \"\"\"\n",
        "        # make rgb values have a range of [0,1]\n",
        "        state = np.array(state)/255.0\n",
        "        # convert state to tensor object and put on GPU\n",
        "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
        "        # making sure gradients aren't saved for the following calculations\n",
        "        with torch.no_grad():\n",
        "            # get action-state values using the foward pass of the network\n",
        "            qs = self.policy_network(state)\n",
        "            # get max action\n",
        "            _, action = qs.max(1)\n",
        "            # return action from tensor object\n",
        "            return action.item()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ohTbKRtCDeO9"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y-CMxm5EDUZ-",
        "outputId": "79499769-3ef6-4ce6-aa61-c2afcf8a0438"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "********************************************************\n",
            "steps: 8111\n",
            "episodes: 10\n",
            "mean 100 episode reward: -20.8\n",
            "% time spent exploring: 91\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 17152\n",
            "episodes: 20\n",
            "mean 100 episode reward: -20.5\n",
            "% time spent exploring: 83\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 26276\n",
            "episodes: 30\n",
            "mean 100 episode reward: -20.5\n",
            "% time spent exploring: 73\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 35395\n",
            "episodes: 40\n",
            "mean 100 episode reward: -20.5\n",
            "% time spent exploring: 64\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 44780\n",
            "episodes: 50\n",
            "mean 100 episode reward: -20.5\n",
            "% time spent exploring: 55\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 53501\n",
            "episodes: 60\n",
            "mean 100 episode reward: -20.5\n",
            "% time spent exploring: 47\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 62932\n",
            "episodes: 70\n",
            "mean 100 episode reward: -20.4\n",
            "% time spent exploring: 37\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 72373\n",
            "episodes: 80\n",
            "mean 100 episode reward: -20.4\n",
            "% time spent exploring: 28\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 81385\n",
            "episodes: 90\n",
            "mean 100 episode reward: -20.4\n",
            "% time spent exploring: 19\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 91839\n",
            "episodes: 100\n",
            "mean 100 episode reward: -20.3\n",
            "% time spent exploring: 9\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 103988\n",
            "episodes: 110\n",
            "mean 100 episode reward: -20.0\n",
            "% time spent exploring: 1\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 119439\n",
            "episodes: 120\n",
            "mean 100 episode reward: -19.6\n",
            "% time spent exploring: 1\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 132893\n",
            "episodes: 130\n",
            "mean 100 episode reward: -19.3\n",
            "% time spent exploring: 1\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 147701\n",
            "episodes: 140\n",
            "mean 100 episode reward: -18.9\n",
            "% time spent exploring: 1\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 163975\n",
            "episodes: 150\n",
            "mean 100 episode reward: -18.8\n",
            "% time spent exploring: 1\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 179700\n",
            "episodes: 160\n",
            "mean 100 episode reward: -18.4\n",
            "% time spent exploring: 1\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 194440\n",
            "episodes: 170\n",
            "mean 100 episode reward: -18.2\n",
            "% time spent exploring: 1\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 213902\n",
            "episodes: 180\n",
            "mean 100 episode reward: -17.6\n",
            "% time spent exploring: 1\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 235560\n",
            "episodes: 190\n",
            "mean 100 episode reward: -16.4\n",
            "% time spent exploring: 1\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 259086\n",
            "episodes: 200\n",
            "mean 100 episode reward: -15.4\n",
            "% time spent exploring: 1\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 285554\n",
            "episodes: 210\n",
            "mean 100 episode reward: -14.2\n",
            "% time spent exploring: 1\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 311845\n",
            "episodes: 220\n",
            "mean 100 episode reward: -13.0\n",
            "% time spent exploring: 1\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 342128\n",
            "episodes: 230\n",
            "mean 100 episode reward: -11.4\n",
            "% time spent exploring: 1\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 372208\n",
            "episodes: 240\n",
            "mean 100 episode reward: -9.3\n",
            "% time spent exploring: 1\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 400164\n",
            "episodes: 250\n",
            "mean 100 episode reward: -6.8\n",
            "% time spent exploring: 1\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 427808\n",
            "episodes: 260\n",
            "mean 100 episode reward: -4.3\n",
            "% time spent exploring: 1\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 456122\n",
            "episodes: 270\n",
            "mean 100 episode reward: -1.7\n",
            "% time spent exploring: 1\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 480981\n",
            "episodes: 280\n",
            "mean 100 episode reward: 0.9\n",
            "% time spent exploring: 1\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 503891\n",
            "episodes: 290\n",
            "mean 100 episode reward: 3.2\n",
            "% time spent exploring: 1\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 522969\n",
            "episodes: 300\n",
            "mean 100 episode reward: 6.0\n",
            "% time spent exploring: 1\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 544059\n",
            "episodes: 310\n",
            "mean 100 episode reward: 8.0\n",
            "% time spent exploring: 1\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 563694\n",
            "episodes: 320\n",
            "mean 100 episode reward: 10.2\n",
            "% time spent exploring: 1\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 581833\n",
            "episodes: 330\n",
            "mean 100 episode reward: 12.3\n",
            "% time spent exploring: 1\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 599868\n",
            "episodes: 340\n",
            "mean 100 episode reward: 13.8\n",
            "% time spent exploring: 1\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 617539\n",
            "episodes: 350\n",
            "mean 100 episode reward: 15.0\n",
            "% time spent exploring: 1\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 635300\n",
            "episodes: 360\n",
            "mean 100 episode reward: 15.6\n",
            "% time spent exploring: 1\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 655341\n",
            "episodes: 370\n",
            "mean 100 episode reward: 16.2\n",
            "% time spent exploring: 1\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 673949\n",
            "episodes: 380\n",
            "mean 100 episode reward: 16.8\n",
            "% time spent exploring: 1\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 692344\n",
            "episodes: 390\n",
            "mean 100 episode reward: 17.2\n",
            "% time spent exploring: 1\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 710955\n",
            "episodes: 400\n",
            "mean 100 episode reward: 17.3\n",
            "% time spent exploring: 1\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 729221\n",
            "episodes: 410\n",
            "mean 100 episode reward: 17.6\n",
            "% time spent exploring: 1\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 747241\n",
            "episodes: 420\n",
            "mean 100 episode reward: 17.7\n",
            "% time spent exploring: 1\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 766026\n",
            "episodes: 430\n",
            "mean 100 episode reward: 17.6\n",
            "% time spent exploring: 1\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 784078\n",
            "episodes: 440\n",
            "mean 100 episode reward: 17.6\n",
            "% time spent exploring: 1\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 801839\n",
            "episodes: 450\n",
            "mean 100 episode reward: 17.6\n",
            "% time spent exploring: 1\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 819841\n",
            "episodes: 460\n",
            "mean 100 episode reward: 18.1\n",
            "% time spent exploring: 1\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 837242\n",
            "episodes: 470\n",
            "mean 100 episode reward: 18.5\n",
            "% time spent exploring: 1\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 856210\n",
            "episodes: 480\n",
            "mean 100 episode reward: 18.5\n",
            "% time spent exploring: 1\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 877025\n",
            "episodes: 490\n",
            "mean 100 episode reward: 18.1\n",
            "% time spent exploring: 1\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 895127\n",
            "episodes: 500\n",
            "mean 100 episode reward: 18.1\n",
            "% time spent exploring: 1\n",
            "********************************************************\n"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "\n",
        "    hyper_params = {\n",
        "        \"seed\": 42,  # which seed to use\n",
        "        \"env\": \"PongNoFrameskip-v4\",  # name of the game\n",
        "        \"replay-buffer-size\": int(5e3),  # replay buffer size\n",
        "        \"learning-rate\": 1e-4,  # learning rate for Adam optimizer\n",
        "        \"discount-factor\": 0.99,  # discount factor\n",
        "        \"num-steps\": int(1e6),  # total number of steps to run the environment for\n",
        "        \"batch-size\": 32,  # number of transitions to optimize at the same time\n",
        "        \"learning-starts\": 10000,  # number of steps before learning starts\n",
        "        \"learning-freq\": 1,  # number of iterations between every optimization step\n",
        "        \"use-double-dqn\": False,  # use double deep Q-learning\n",
        "        \"target-update-freq\": 1000,  # number of iterations between every target network update\n",
        "        \"eps-start\": 1.0,  # e-greedy start threshold\n",
        "        \"eps-end\": 0.01,  # e-greedy end threshold\n",
        "        \"eps-fraction\": 0.1,  # fraction of num-steps\n",
        "        \"print-freq\": 10,\n",
        "    }\n",
        "\n",
        "    np.random.seed(hyper_params[\"seed\"])\n",
        "    random.seed(hyper_params[\"seed\"])\n",
        "\n",
        "    assert \"NoFrameskip\" in hyper_params[\"env\"], \"Require environment with no frameskip\"\n",
        "    \n",
        "    env = gym.make(hyper_params[\"env\"])\n",
        "    env.seed(hyper_params[\"seed\"])\n",
        "\n",
        "    # TODO Pick Gym wrappers to use\n",
        "    env = NoopResetEnv(env, noop_max=30)\n",
        "    env = MaxAndSkipEnv(env, skip=4)\n",
        "    env = EpisodicLifeEnv(env)\n",
        "    env = FireResetEnv(env)\n",
        "    env = WarpFrame(env) #-> warp frame to 84x84\n",
        "    env = PyTorchFrame(env) # turn to channel x height xwidth dimension for pytorch \n",
        "    env = FrameStack(env,4) #take the last 4 frames \n",
        "    # env = ClipRewardEnv(env)\n",
        "    # record video every 50 episodes\n",
        "    env = gym.wrappers.Monitor(env, '/content/drive/Shareddrives/Reinforcement_Learning/videos/', video_callable=lambda episode_id: episode_id % 50 == 0, force=True)\n",
        "    \n",
        "    replay_buffer = ReplayBuffer(hyper_params[\"replay-buffer-size\"])\n",
        "\n",
        "    # TODO Create dqn agent\n",
        "    agent = DQNAgent(env.observation_space,\n",
        "        env.action_space,\n",
        "        replay_buffer,\n",
        "        use_double_dqn=hyper_params[\"use-double-dqn\"],\n",
        "        lr=hyper_params[\"learning-rate\"],\n",
        "        batch_size= hyper_params[\"batch-size\"],\n",
        "        gamma=hyper_params[\"discount-factor\"])\n",
        "\n",
        "    eps_timesteps = hyper_params[\"eps-fraction\"] * float(hyper_params[\"num-steps\"])\n",
        "    episode_rewards = [0.0]\n",
        "\n",
        "    state = env.reset()\n",
        "    for t in range(hyper_params[\"num-steps\"]):\n",
        "        fraction = min(1.0, float(t) / eps_timesteps)\n",
        "        eps_threshold = hyper_params[\"eps-start\"] + fraction * (\n",
        "            hyper_params[\"eps-end\"] - hyper_params[\"eps-start\"]\n",
        "        )\n",
        "        sample = random.random()\n",
        "        # TODO done i think lol\n",
        "        #  select random action if sample is less equal than eps_threshold\n",
        "        # take step in env\n",
        "        # add state, action, reward, next_state, float(done) to reply memory - cast done to float\n",
        "        # add reward to episode_reward\n",
        "        # action = None\n",
        "        if sample < eps_threshold:\n",
        "            action = random.randrange(env.action_space.n)\n",
        "        else:\n",
        "            action = agent.act(state)\n",
        "\n",
        "        #take next step\n",
        "        next_state, reward, done, _ = env.step(action) \n",
        "        agent.replay_buffer.add(state, action, reward, next_state, float(done))\n",
        "        state = next_state\n",
        "\n",
        "        episode_rewards[-1] += reward\n",
        "        if done:\n",
        "            state = env.reset()\n",
        "            episode_rewards.append(0.0)\n",
        "\n",
        "        if (\n",
        "            t > hyper_params[\"learning-starts\"]\n",
        "            and t % hyper_params[\"learning-freq\"] == 0\n",
        "        ):\n",
        "            agent.optimise_td_loss()\n",
        "\n",
        "        if (\n",
        "            t > hyper_params[\"learning-starts\"]\n",
        "            and t % hyper_params[\"target-update-freq\"] == 0\n",
        "        ):\n",
        "            agent.update_target_network()\n",
        "\n",
        "        num_episodes = len(episode_rewards)\n",
        "\n",
        "        if (\n",
        "            done\n",
        "            and hyper_params[\"print-freq\"] is not None\n",
        "            and len(episode_rewards) % hyper_params[\"print-freq\"] == 0\n",
        "        ):\n",
        "            mean_100ep_reward = round(np.mean(episode_rewards[-101:-1]), 1)\n",
        "            print(\"********************************************************\")\n",
        "            print(\"steps: {}\".format(t))\n",
        "            print(\"episodes: {}\".format(num_episodes))\n",
        "            print(\"mean 100 episode reward: {}\".format(mean_100ep_reward))\n",
        "            print(\"% time spent exploring: {}\".format(int(100 * eps_threshold)))\n",
        "            print(\"********************************************************\")\n",
        "            np.savetxt('/content/drive/Shareddrives/Reinforcement_Learning/rewards.csv', episode_rewards)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ZagncRvCZKJP"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "rewards = pd.read_csv(\"/content/drive/Shareddrives/Reinforcement_Learning/rewards.csv\") \n",
        "rewards = rewards.to_numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "id": "ouR2ApCCZPew",
        "outputId": "a47fe98b-1338-4139-f646-fafb15eb9d0c"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot([i for i in range(len(rewards))], rewards)\n",
        "plt.xlabel('Episode no.')\n",
        "plt.ylabel('Reward of DQN')\n",
        "plt.show()\n",
        "print()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BxsPQM-gWWRf",
        "outputId": "e16ad7c4-d108-4d30-e238-cb22ee208248"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t6JLGpYF4rz8"
      },
      "source": [
        "# End"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}